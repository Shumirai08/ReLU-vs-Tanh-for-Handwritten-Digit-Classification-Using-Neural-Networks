Handwritten digit recognition is a fundamental problem in the field of artificial intelligence and computer vision, widely used as a benchmark for evaluating neural network performance. This project investigates the effectiveness of artificial neural networks in classifying handwritten digits from the MNIST dataset, with a particular focus on comparing the performance of ReLU (Rectified Linear Unit) and Tanh (Hyperbolic Tangent) activation functions. The proposed model employs a simple feedforward neural network architecture consisting of an input layer, a single hidden layer, and an output layer with SoftMax activation for multi-class classification.

The MNIST dataset, containing 70,000 grayscale images of handwritten digits, is preprocessed through normalization and one-hot encoding to ensure efficient learning. Two separate models are trained using identical architectures while varying only the activation function in the hidden layer. Model performance is evaluated using accuracy, loss metrics, confusion matrices, and classification reports. Experimental results demonstrate that both activation functions achieve high classification accuracy, with ReLU slightly outperforming Tanh in terms of convergence speed and final loss, while Tanh shows stable performance on normalized and balanced data.

The findings highlight the impact of activation function choice on neural network learning behavior and performance. This study concludes that while both ReLU and Tanh are effective for handwritten digit recognition, ReLU provides superior computational efficiency and generalization, making it more suitable for deep learning applications. The project also discusses ethical considerations and real-world implications of neural network-based image classification systems.
